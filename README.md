# Идеи для латентников

## Пробинг

### Пробинг на энкодере

* Пробинг по разным attention heads

* Предсказывать структурные дескрипторы из хидденов (у разных групп разные сдвиги)


### Пробинг на декодере

* Предсказывать структурные дескрипторы из хидденов (в какой-то момент определяет число азотов, в какой-то -- количество колец, в какой-то -- число доноров-акцепторов в водородной связи), смотрим где оно собирает подструктуры каких размеров (1 атом, 2-3 атома, 6 < атомов)

* Пробинг lm heads (приложить папиру)

* Пробинг по разным attention heads

* То же самое но нелинейно (пробинг xgboostом)

## Something That Can Be Called Geometry

* PCA (Eigenvalue Early Enrichment)

* Папира с протеинами

* Производная изотропии по слоям (We can observe the dynamics of anisotropy across layers. As I have read, in transformer-based
architectures, anisotropy usually increases from the initial to the final layers)

* Какая метрика пиздатая по пространству (аля папира с протеинами)

* Можно посравнивать эмбеддинги в двух параллельных энкодерах, есть ли кластеризация по близости, как построить отображение из пространства одного энкодера в пространство другого?

* Кластеризуем эмбеддинги экнодера через knn+силуэтт и посмотреть есть ли корреляция между номером кластера и какими-то структурными фичами и мб элмаг фичами, посмотреть на химсдвиги групп и кластеры

* Point Patchiness как липшецевость между спектрами и между эмбеддингами, расстояние -- Вассерштайн

* Reconstruction Skew -- надо погуглить формулы по норм папирам

* Centroid Distribution + изотропность: как «семантические центры» сидят в латенте (дисперсия центроид как в энкодере так и в декодере)

## Анализ attention heads

* Можно посмотреть на классификации голов декодера как в китайской папире, выдумать как классифицировать их автоматически (эвристики или выучить) и понять как классификация скоррелирована с идейностью по токенам

## 

* Sparce ae над декодером + стиринг

